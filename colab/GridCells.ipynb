{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GridCells.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2OkwkLcfFjLG","colab_type":"text"},"source":["# Pip Installs"]},{"cell_type":"code","metadata":{"id":"xsDL0Q36FmUl","colab_type":"code","outputId":"4cd439a3-cff1-41a9-ec5f-687cc327005e","executionInfo":{"status":"ok","timestamp":1573576964334,"user_tz":300,"elapsed":25943,"user":{"displayName":"Salim M'Jahad","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCFL4tbM0hSOkSuMWfn2CYHleHxlhi9cylY8gPIOQ=s64","userId":"06219843532523503157"}},"colab":{"base_uri":"https://localhost:8080/","height":717}},"source":["!pip install --upgrade numpy\n","!pip install --upgrade tensorflow==1.12.3\n","!pip install --upgrade dm-sonnet==1.27\n","!pip install --upgrade scipy\n","!pip install --upgrade matplotlib\n","!pip install --upgrade tensorflow-probability==0.8.0\n","!pip install --upgrade wrapt==1.9.0"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.17.4)\n","Requirement already up-to-date: tensorflow==1.12.3 in /usr/local/lib/python3.6/dist-packages (1.12.3)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.3) (3.10.0)\n","Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.3) (1.1.0)\n","Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.3) (0.8.0)\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.3) (1.17.4)\n","Requirement already satisfied, skipping upgrade: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.3) (1.12.2)\n","Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.3) (1.1.0)\n","Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.3) (0.8.1)\n","Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.3) (0.2.2)\n","Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.3) (1.0.8)\n","Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.3) (0.33.6)\n","Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.3) (1.12.0)\n","Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.3) (1.15.0)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12.3) (41.4.0)\n","Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.3) (3.1.1)\n","Requirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.3) (0.16.0)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12.3) (2.8.0)\n","Requirement already up-to-date: dm-sonnet==1.27 in /usr/local/lib/python3.6/dist-packages (1.27)\n","Requirement already satisfied, skipping upgrade: contextlib2 in /usr/local/lib/python3.6/dist-packages (from dm-sonnet==1.27) (0.5.5)\n","Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from dm-sonnet==1.27) (0.8.1)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from dm-sonnet==1.27) (1.12.0)\n","Requirement already satisfied, skipping upgrade: semantic-version in /usr/local/lib/python3.6/dist-packages (from dm-sonnet==1.27) (2.8.2)\n","Requirement already up-to-date: scipy in /usr/local/lib/python3.6/dist-packages (1.3.2)\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.17.4)\n","Requirement already up-to-date: matplotlib in /usr/local/lib/python3.6/dist-packages (3.1.1)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.6.1)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.4)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.17.4)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.4.0)\n","Requirement already up-to-date: tensorflow-probability==0.8.0 in /usr/local/lib/python3.6/dist-packages (0.8.0)\n","Requirement already satisfied, skipping upgrade: cloudpickle==1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (1.1.1)\n","Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (1.12.0)\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (1.17.4)\n","Requirement already satisfied, skipping upgrade: gast<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (0.2.2)\n","Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (4.4.1)\n","Requirement already up-to-date: wrapt==1.9.0 in /usr/local/lib/python3.6/dist-packages (1.9.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZOUPtyQDDetI","colab_type":"text"},"source":["# dataset_reader.py"]},{"cell_type":"code","metadata":{"id":"9BOpnzjhDDCO","colab_type":"code","colab":{}},"source":["# Copyright 2018 Google LLC\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\n","\"\"\"Minimal queue based TFRecord reader for the Grid Cell paper.\"\"\"\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import collections\n","import os\n","import tensorflow as tf\n","nest = tf.contrib.framework.nest\n","\n","DatasetInfo = collections.namedtuple(\n","    'DatasetInfo', ['basepath', 'size', 'sequence_length', 'coord_range'])\n","\n","_DATASETS = dict(\n","    square_room=DatasetInfo(\n","        basepath='square_room_100steps_2.2m_1000000',\n","        size=100,\n","        sequence_length=100,\n","        coord_range=((-1.1, 1.1), (-1.1, 1.1))),)\n","\n","\n","def _get_dataset_files(dateset_info, root):\n","  \"\"\"Generates lists of files for a given dataset version.\"\"\"\n","  basepath = dateset_info.basepath\n","  base = os.path.join(root, basepath)\n","  num_files = dateset_info.size\n","  template = '{:0%d}-of-{:0%d}.tfrecord' % (4, 4)\n","  return [\n","      os.path.join(base, template.format(i, num_files - 1))\n","      for i in range(num_files)\n","  ]\n","\n","\n","class DataReader(object):\n","  \"\"\"Minimal queue based TFRecord reader.\n","  You can use this reader to load the datasets used to train the grid cell\n","  network in the 'Vector-based Navigation using Grid-like Representations\n","  in Artificial Agents' paper.\n","  See README.md for a description of the datasets and an example of how to use\n","  the reader.\n","  \"\"\"\n","\n","  def __init__(\n","      self,\n","      dataset,\n","      root,\n","      # Queue params\n","      num_threads=4,\n","      capacity=256,\n","      min_after_dequeue=128,\n","      seed=None):\n","    \"\"\"Instantiates a DataReader object and sets up queues for data reading.\n","    Args:\n","      dataset: string, one of ['jaco', 'mazes', 'rooms_ring_camera',\n","        'rooms_free_camera_no_object_rotations',\n","        'rooms_free_camera_with_object_rotations', 'shepard_metzler_5_parts',\n","        'shepard_metzler_7_parts'].\n","      root: string, path to the root folder of the data.\n","      num_threads: (optional) integer, number of threads used to feed the reader\n","        queues, defaults to 4.\n","      capacity: (optional) integer, capacity of the underlying\n","        RandomShuffleQueue, defaults to 256.\n","      min_after_dequeue: (optional) integer, min_after_dequeue of the underlying\n","        RandomShuffleQueue, defaults to 128.\n","      seed: (optional) integer, seed for the random number generators used in\n","        the reader.\n","    Raises:\n","      ValueError: if the required version does not exist;\n","    \"\"\"\n","\n","    if dataset not in _DATASETS:\n","      raise ValueError('Unrecognized dataset {} requested. Available datasets '\n","                       'are {}'.format(dataset, _DATASETS.keys()))\n","\n","    self._dataset_info = _DATASETS[dataset]\n","    self._steps = _DATASETS[dataset].sequence_length\n","\n","    with tf.device('/cpu'):\n","      file_names = _get_dataset_files(self._dataset_info, root)\n","      filename_queue = tf.train.string_input_producer(file_names, seed=seed)\n","      reader = tf.TFRecordReader()\n","\n","      read_ops = [\n","          self._make_read_op(reader, filename_queue) for _ in range(num_threads)\n","      ]\n","      dtypes = nest.map_structure(lambda x: x.dtype, read_ops[0])\n","      shapes = nest.map_structure(lambda x: x.shape[1:], read_ops[0])\n","\n","      self._queue = tf.RandomShuffleQueue(\n","          capacity=capacity,\n","          min_after_dequeue=min_after_dequeue,\n","          dtypes=dtypes,\n","          shapes=shapes,\n","          seed=seed)\n","\n","      enqueue_ops = [self._queue.enqueue_many(op) for op in read_ops]\n","      tf.train.add_queue_runner(tf.train.QueueRunner(self._queue, enqueue_ops))\n","\n","  def read(self, batch_size):\n","    \"\"\"Reads batch_size.\"\"\"\n","    in_pos, in_hd, ego_vel, target_pos, target_hd = self._queue.dequeue_many(\n","        batch_size)\n","    return in_pos, in_hd, ego_vel, target_pos, target_hd\n","\n","  def get_coord_range(self):\n","    return self._dataset_info.coord_range\n","\n","  def _make_read_op(self, reader, filename_queue):\n","    \"\"\"Instantiates the ops used to read and parse the data into tensors.\"\"\"\n","    _, raw_data = reader.read_up_to(filename_queue, num_records=64)\n","    feature_map = {\n","        'init_pos':\n","            tf.FixedLenFeature(shape=[2], dtype=tf.float32),\n","        'init_hd':\n","            tf.FixedLenFeature(shape=[1], dtype=tf.float32),\n","        'ego_vel':\n","            tf.FixedLenFeature(\n","                shape=[self._dataset_info.sequence_length, 3],\n","                dtype=tf.float32),\n","        'target_pos':\n","            tf.FixedLenFeature(\n","                shape=[self._dataset_info.sequence_length, 2],\n","                dtype=tf.float32),\n","        'target_hd':\n","            tf.FixedLenFeature(\n","                shape=[self._dataset_info.sequence_length, 1],\n","                dtype=tf.float32),\n","    }\n","    example = tf.parse_example(raw_data, feature_map)\n","    batch = [\n","        example['init_pos'], example['init_hd'],\n","        example['ego_vel'][:, :self._steps, :],\n","        example['target_pos'][:, :self._steps, :],\n","        example['target_hd'][:, :self._steps, :]\n","    ]\n","    return batch"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9y92HlnXDk9T","colab_type":"text"},"source":["# ensembles.py"]},{"cell_type":"code","metadata":{"id":"iKOJPquJDLWl","colab_type":"code","colab":{}},"source":["# Copyright 2018 Google LLC\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\n","\"\"\"Ensembles of place and head direction cells.\n","These classes provide the targets for the training of grid-cell networks.\n","\"\"\"\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import numpy as np\n","import tensorflow as tf\n","\n","\n","def one_hot_max(x, axis=-1):\n","  \"\"\"Compute one-hot vectors setting to one the index with the maximum value.\"\"\"\n","  return tf.one_hot(tf.argmax(x, axis=axis),\n","                    depth=x.get_shape()[-1],\n","                    dtype=x.dtype)\n","\n","\n","def softmax(x, axis=-1):\n","  \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n","  return tf.nn.softmax(x, dim=axis)\n","\n","\n","def softmax_sample(x):\n","  \"\"\"Sample the categorical distribution from logits and sample it.\"\"\"\n","  dist = tf.contrib.distributions.OneHotCategorical(logits=x, dtype=tf.float32)\n","  return dist.sample()\n","\n","\n","class CellEnsemble(object):\n","  \"\"\"Abstract parent class for place and head direction cell ensembles.\"\"\"\n","\n","  def __init__(self, n_cells, soft_targets, soft_init):\n","    self.n_cells = n_cells\n","    if soft_targets not in [\"softmax\", \"voronoi\", \"sample\", \"normalized\"]:\n","      raise ValueError\n","    else:\n","      self.soft_targets = soft_targets\n","    # Provide initialization of LSTM in the same way as targets if not specified\n","    # i.e one-hot if targets are Voronoi\n","    if soft_init is None:\n","      self.soft_init = soft_targets\n","    else:\n","      if soft_init not in [\n","          \"softmax\", \"voronoi\", \"sample\", \"normalized\", \"zeros\"\n","      ]:\n","        raise ValueError\n","      else:\n","        self.soft_init = soft_init\n","\n","  def get_targets(self, x):\n","    \"\"\"Type of target.\"\"\"\n","\n","    if self.soft_targets == \"normalized\":\n","      targets = tf.exp(self.unnor_logpdf(x))\n","    elif self.soft_targets == \"softmax\":\n","      lp = self.log_posterior(x)\n","      targets = softmax(lp)\n","    elif self.soft_targets == \"sample\":\n","      lp = self.log_posterior(x)\n","      targets = softmax_sample(lp)\n","    elif self.soft_targets == \"voronoi\":\n","      lp = self.log_posterior(x)\n","      targets = one_hot_max(lp)\n","    return targets\n","\n","  def get_init(self, x):\n","    \"\"\"Type of initialisation.\"\"\"\n","\n","    if self.soft_init == \"normalized\":\n","      init = tf.exp(self.unnor_logpdf(x))\n","    elif self.soft_init == \"softmax\":\n","      lp = self.log_posterior(x)\n","      init = softmax(lp)\n","    elif self.soft_init == \"sample\":\n","      lp = self.log_posterior(x)\n","      init = softmax_sample(lp)\n","    elif self.soft_init == \"voronoi\":\n","      lp = self.log_posterior(x)\n","      init = one_hot_max(lp)\n","    elif self.soft_init == \"zeros\":\n","      init = tf.zeros_like(self.unnor_logpdf(x))\n","    return init\n","\n","  def loss(self, predictions, targets):\n","    \"\"\"Loss.\"\"\"\n","\n","    if self.soft_targets == \"normalized\":\n","      smoothing = 1e-2\n","      loss = tf.nn.sigmoid_cross_entropy_with_logits(\n","          labels=(1. - smoothing) * targets + smoothing * 0.5,\n","          logits=predictions,\n","          name=\"ensemble_loss\")\n","      loss = tf.reduce_mean(loss, axis=-1)\n","    else:\n","      loss = tf.nn.softmax_cross_entropy_with_logits(\n","          labels=targets,\n","          logits=predictions,\n","          name=\"ensemble_loss\")\n","    return loss\n","\n","  def log_posterior(self, x):\n","    logp = self.unnor_logpdf(x)\n","    log_posteriors = logp - tf.reduce_logsumexp(logp, axis=2, keep_dims=True)\n","    return log_posteriors\n","\n","\n","class PlaceCellEnsemble(CellEnsemble):\n","  \"\"\"Calculates the dist over place cells given an absolute position.\"\"\"\n","\n","  def __init__(self, n_cells, stdev=0.35, pos_min=-5, pos_max=5, seed=None,\n","               soft_targets=None, soft_init=None):\n","    super(PlaceCellEnsemble, self).__init__(n_cells, soft_targets, soft_init)\n","    # Create a random MoG with fixed cov over the position (Nx2)\n","    rs = np.random.RandomState(seed)\n","    self.means = rs.uniform(pos_min, pos_max, size=(self.n_cells, 2))\n","    self.variances = np.ones_like(self.means) * stdev**2\n","\n","  def unnor_logpdf(self, trajs):\n","    # Output the probability of each component at each point (BxTxN)\n","    diff = trajs[:, :, tf.newaxis, :] - self.means[np.newaxis, np.newaxis, ...]\n","    unnor_logp = -0.5 * tf.reduce_sum((diff**2)/ self.variances, axis=-1)\n","    return unnor_logp\n","\n","\n","class HeadDirectionCellEnsemble(CellEnsemble):\n","  \"\"\"Calculates the dist over HD cells given an absolute angle.\"\"\"\n","\n","  def __init__(self, n_cells, concentration=20, seed=None,\n","               soft_targets=None, soft_init=None):\n","    super(HeadDirectionCellEnsemble, self).__init__(n_cells,\n","                                                    soft_targets,\n","                                                    soft_init)\n","    # Create a random Von Mises with fixed cov over the position\n","    rs = np.random.RandomState(seed)\n","    self.means = rs.uniform(-np.pi, np.pi, (n_cells))\n","    self.kappa = np.ones_like(self.means) * concentration\n","\n","  def unnor_logpdf(self, x):\n","    return self.kappa * tf.cos(x - self.means[np.newaxis, np.newaxis, :])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ALgYhKxQDwll","colab_type":"text"},"source":["# model.py"]},{"cell_type":"code","metadata":{"id":"4w2dZSa5f2y8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":241},"outputId":"71cd93bc-18a8-451f-a0fb-305e3c0f6370","executionInfo":{"status":"ok","timestamp":1573577018213,"user_tz":300,"elapsed":6018,"user":{"displayName":"Salim M'Jahad","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCFL4tbM0hSOkSuMWfn2CYHleHxlhi9cylY8gPIOQ=s64","userId":"06219843532523503157"}}},"source":["!pip install --upgrade tensorflow-probability==0.5.0"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-probability==0.5.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/ca/6f213618b5f7d0bf6139e6ec928d412a5ca14e4776adfd41a59c74a34021/tensorflow_probability-0.5.0-py2.py3-none-any.whl (680kB)\n","\r\u001b[K     |▌                               | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 3.4MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 686kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.17.4)\n","Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.12.0)\n","\u001b[31mERROR: tensorflow-gan 2.0.0 has requirement tensorflow-probability>=0.7, but you'll have tensorflow-probability 0.5.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensor2tensor 1.14.1 has requirement tensorflow-probability==0.7.0, but you'll have tensorflow-probability 0.5.0 which is incompatible.\u001b[0m\n","Installing collected packages: tensorflow-probability\n","  Found existing installation: tensorflow-probability 0.8.0\n","    Uninstalling tensorflow-probability-0.8.0:\n","      Successfully uninstalled tensorflow-probability-0.8.0\n","Successfully installed tensorflow-probability-0.5.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ucHwm2DTDLfs","colab_type":"code","outputId":"5a552c95-9acd-4dc7-e8ca-abf7c2d006bd","executionInfo":{"status":"ok","timestamp":1573577022211,"user_tz":300,"elapsed":1021,"user":{"displayName":"Salim M'Jahad","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCFL4tbM0hSOkSuMWfn2CYHleHxlhi9cylY8gPIOQ=s64","userId":"06219843532523503157"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["# Copyright 2018 Google LLC\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\n","\"\"\"Model for grid cells supervised training.\n","\"\"\"\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import numpy\n","import sonnet as snt\n","import tensorflow as tf\n","\n","\n","def displaced_linear_initializer(input_size, displace, dtype=tf.float32):\n","  stddev = 1. / numpy.sqrt(input_size)\n","  return tf.truncated_normal_initializer(\n","      mean=displace*stddev, stddev=stddev, dtype=dtype)\n","\n","\n","class GridCellsRNNCell(snt.RNNCore):\n","  \"\"\"LSTM core implementation for the grid cell network.\"\"\"\n","\n","  def __init__(self,\n","               target_ensembles,\n","               nh_lstm,\n","               nh_bottleneck,\n","               nh_embed=None,\n","               dropoutrates_bottleneck=None,\n","               bottleneck_weight_decay=0.0,\n","               bottleneck_has_bias=False,\n","               init_weight_disp=0.0,\n","               name=\"grid_cells_core\"):\n","    \"\"\"Constructor of the RNN cell.\n","    Args:\n","      target_ensembles: Targets, place cells and head direction cells.\n","      nh_lstm: Size of LSTM cell.\n","      nh_bottleneck: Size of the linear layer between LSTM output and output.\n","      nh_embed: Number of hiddens between input and LSTM input.\n","      dropoutrates_bottleneck: Iterable of keep rates (0,1]. The linear layer is\n","        partitioned into as many groups as the len of this parameter.\n","      bottleneck_weight_decay: Weight decay used in the bottleneck layer.\n","      bottleneck_has_bias: If the bottleneck has a bias.\n","      init_weight_disp: Displacement in the weights initialisation.\n","      name: the name of the module.\n","    \"\"\"\n","    super(GridCellsRNNCell, self).__init__(name=name)\n","    self._target_ensembles = target_ensembles\n","    self._nh_embed = nh_embed\n","    self._nh_lstm = nh_lstm\n","    self._nh_bottleneck = nh_bottleneck\n","    self._dropoutrates_bottleneck = dropoutrates_bottleneck\n","    self._bottleneck_weight_decay = bottleneck_weight_decay\n","    self._bottleneck_has_bias = bottleneck_has_bias\n","    self._init_weight_disp = init_weight_disp\n","    self.training = False\n","    with self._enter_variable_scope():\n","      self._lstm = snt.LSTM(self._nh_lstm)\n","\n","  def _build(self, inputs, prev_state):\n","    \"\"\"Build the module.\n","    Args:\n","      inputs: Egocentric velocity (BxN)\n","      prev_state: Previous state of the recurrent network\n","    Returns:\n","      ((predictions, bottleneck, lstm_outputs), next_state)\n","      The predictions\n","    \"\"\"\n","    conc_inputs = tf.concat(inputs, axis=1, name=\"conc_inputs\")\n","    # Embedding layer\n","    lstm_inputs = conc_inputs\n","    # LSTM\n","    lstm_output, next_state = self._lstm(lstm_inputs, prev_state)\n","    # Bottleneck\n","    bottleneck = snt.Linear(self._nh_bottleneck,\n","                            use_bias=self._bottleneck_has_bias,\n","                            regularizers={\n","                                \"w\": tf.contrib.layers.l2_regularizer(\n","                                    self._bottleneck_weight_decay)},\n","                            name=\"bottleneck\")(lstm_output)\n","    if self.training and self._dropoutrates_bottleneck is not None:\n","      tf.logging.info(\"Adding dropout layers\")\n","      n_scales = len(self._dropoutrates_bottleneck)\n","      scale_pops = tf.split(bottleneck, n_scales, axis=1)\n","      dropped_pops = [tf.nn.dropout(pop, rate, name=\"dropout\")\n","                      for rate, pop in zip(self._dropoutrates_bottleneck,\n","                                           scale_pops)]\n","      bottleneck = tf.concat(dropped_pops, axis=1)\n","    # Outputs\n","    ens_outputs = [snt.Linear(\n","        ens.n_cells,\n","        regularizers={\n","            \"w\": tf.contrib.layers.l2_regularizer(\n","                self._bottleneck_weight_decay)},\n","        initializers={\n","            \"w\": displaced_linear_initializer(self._nh_bottleneck,\n","                                              self._init_weight_disp,\n","                                              dtype=tf.float32)},\n","        name=\"pc_logits\")(bottleneck)\n","                   for ens in self._target_ensembles]\n","    return (ens_outputs, bottleneck, lstm_output), tuple(list(next_state))\n","\n","  @property\n","  def state_size(self):\n","    \"\"\"Returns a description of the state size, without batch dimension.\"\"\"\n","    return self._lstm.state_size\n","\n","  @property\n","  def output_size(self):\n","    \"\"\"Returns a description of the output size, without batch dimension.\"\"\"\n","    return tuple([ens.n_cells for ens in self._target_ensembles] +\n","                 [self._nh_bottleneck, self._nh_lstm])\n","\n","\n","class GridCellsRNN(snt.AbstractModule):\n","  \"\"\"RNN computes place and head-direction cell predictions from velocities.\"\"\"\n","\n","  def __init__(self, rnn_cell, nh_lstm, name=\"grid_cell_supervised\"):\n","    super(GridCellsRNN, self).__init__(name=name)\n","    self._core = rnn_cell\n","    self._nh_lstm = nh_lstm\n","\n","  def _build(self, init_conds, vels, training=False):\n","    \"\"\"Outputs place, and head direction cell predictions from velocity inputs.\n","    Args:\n","      init_conds: Initial conditions given by ensemble activatons, list [BxN_i]\n","      vels:  Translational and angular velocities [BxTxV]\n","      training: Activates and deactivates dropout\n","    Returns:\n","      [logits_i]:\n","        logits_i: Logits predicting i-th ensemble activations (BxTxN_i)\n","    \"\"\"\n","    # Calculate initialization for LSTM. Concatenate pc and hdc activations\n","    concat_init = tf.concat(init_conds, axis=1)\n","\n","    init_lstm_state = snt.Linear(self._nh_lstm, name=\"state_init\")(concat_init)\n","    init_lstm_cell = snt.Linear(self._nh_lstm, name=\"cell_init\")(concat_init)\n","    self._core.training = training\n","\n","    # Run LSTM\n","    output_seq, final_state = tf.nn.dynamic_rnn(cell=self._core,\n","                                                inputs=(vels,),\n","                                                time_major=False,\n","                                                initial_state=(init_lstm_state,\n","                                                               init_lstm_cell))\n","    ens_targets = output_seq[:-2]\n","    bottleneck = output_seq[-2]\n","    lstm_output = output_seq[-1]\n","    # Return\n","    return (ens_targets, bottleneck, lstm_output), final_state\n","\n","  def get_all_variables(self):\n","    return (super(GridCellsRNN, self).get_variables()\n","            + self._core.get_variables())"],"execution_count":15,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sonnet/__init__.py:57: PendingDeprecationWarning: The Spec() class will be removed in 3.1; use SimpleSpec() instead.\n","  version_spec = semantic_version.Spec('>=' + min_version)\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n","  return _inspect.getargspec(target)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"vb49jSO5DzXm","colab_type":"text"},"source":["# scores.py"]},{"cell_type":"code","metadata":{"id":"6s_NPXowDLmI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"da28b988-3231-4449-9eaa-deb519f7892c","executionInfo":{"status":"ok","timestamp":1573577028297,"user_tz":300,"elapsed":1066,"user":{"displayName":"Salim M'Jahad","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCFL4tbM0hSOkSuMWfn2CYHleHxlhi9cylY8gPIOQ=s64","userId":"06219843532523503157"}}},"source":["# Copyright 2018 Google LLC\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\n","\"\"\"Grid score calculations.\n","\"\"\"\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy.signal\n","\n","\n","def circle_mask(size, radius, in_val=1.0, out_val=0.0):\n","  \"\"\"Calculating the grid scores with different radius.\"\"\"\n","  sz = [math.floor(size[0] / 2), math.floor(size[1] / 2)]\n","  x = np.linspace(-sz[0], sz[1], size[1])\n","  x = np.expand_dims(x, 0)\n","  x = x.repeat(size[0], 0)\n","  y = np.linspace(-sz[0], sz[1], size[1])\n","  y = np.expand_dims(y, 1)\n","  y = y.repeat(size[1], 1)\n","  z = np.sqrt(x**2 + y**2)\n","  z = np.less_equal(z, radius)\n","  vfunc = np.vectorize(lambda b: b and in_val or out_val)\n","  return vfunc(z)\n","\n","\n","class GridScorer(object):\n","  \"\"\"Class for scoring ratemaps given trajectories.\"\"\"\n","\n","  def __init__(self, nbins, coords_range, mask_parameters, min_max=False):\n","    \"\"\"Scoring ratemaps given trajectories.\n","    Args:\n","      nbins: Number of bins per dimension in the ratemap.\n","      coords_range: Environment coordinates range.\n","      mask_parameters: parameters for the masks that analyze the angular\n","        autocorrelation of the 2D autocorrelation.\n","      min_max: Correction.\n","    \"\"\"\n","    self._nbins = nbins\n","    self._min_max = min_max\n","    self._coords_range = coords_range\n","    self._corr_angles = [30, 45, 60, 90, 120, 135, 150]\n","    # Create all masks\n","    self._masks = [(self._get_ring_mask(mask_min, mask_max), (mask_min,\n","                                                              mask_max))\n","                   for mask_min, mask_max in mask_parameters]\n","    # Mask for hiding the parts of the SAC that are never used\n","    self._plotting_sac_mask = circle_mask(\n","        [self._nbins * 2 - 1, self._nbins * 2 - 1],\n","        self._nbins,\n","        in_val=1.0,\n","        out_val=np.nan)\n","\n","  def calculate_ratemap(self, xs, ys, activations, statistic='mean'):\n","    return scipy.stats.binned_statistic_2d(\n","        xs,\n","        ys,\n","        activations,\n","        bins=self._nbins,\n","        statistic=statistic,\n","        range=self._coords_range)[0]\n","\n","  def _get_ring_mask(self, mask_min, mask_max):\n","    n_points = [self._nbins * 2 - 1, self._nbins * 2 - 1]\n","    return (circle_mask(n_points, mask_max * self._nbins) *\n","            (1 - circle_mask(n_points, mask_min * self._nbins)))\n","\n","  def grid_score_60(self, corr):\n","    if self._min_max:\n","      return np.minimum(corr[60], corr[120]) - np.maximum(\n","          corr[30], np.maximum(corr[90], corr[150]))\n","    else:\n","      return (corr[60] + corr[120]) / 2 - (corr[30] + corr[90] + corr[150]) / 3\n","\n","  def grid_score_90(self, corr):\n","    return corr[90] - (corr[45] + corr[135]) / 2\n","\n","  def calculate_sac(self, seq1):\n","    \"\"\"Calculating spatial autocorrelogram.\"\"\"\n","    seq2 = seq1\n","\n","    def filter2(b, x):\n","      stencil = np.rot90(b, 2)\n","      return scipy.signal.convolve2d(x, stencil, mode='full')\n","\n","    seq1 = np.nan_to_num(seq1)\n","    seq2 = np.nan_to_num(seq2)\n","\n","    ones_seq1 = np.ones(seq1.shape)\n","    ones_seq1[np.isnan(seq1)] = 0\n","    ones_seq2 = np.ones(seq2.shape)\n","    ones_seq2[np.isnan(seq2)] = 0\n","\n","    seq1[np.isnan(seq1)] = 0\n","    seq2[np.isnan(seq2)] = 0\n","\n","    seq1_sq = np.square(seq1)\n","    seq2_sq = np.square(seq2)\n","\n","    seq1_x_seq2 = filter2(seq1, seq2)\n","    sum_seq1 = filter2(seq1, ones_seq2)\n","    sum_seq2 = filter2(ones_seq1, seq2)\n","    sum_seq1_sq = filter2(seq1_sq, ones_seq2)\n","    sum_seq2_sq = filter2(ones_seq1, seq2_sq)\n","    n_bins = filter2(ones_seq1, ones_seq2)\n","    n_bins_sq = np.square(n_bins)\n","\n","    std_seq1 = np.power(\n","        np.subtract(\n","            np.divide(sum_seq1_sq, n_bins),\n","            (np.divide(np.square(sum_seq1), n_bins_sq))), 0.5)\n","    std_seq2 = np.power(\n","        np.subtract(\n","            np.divide(sum_seq2_sq, n_bins),\n","            (np.divide(np.square(sum_seq2), n_bins_sq))), 0.5)\n","    covar = np.subtract(\n","        np.divide(seq1_x_seq2, n_bins),\n","        np.divide(np.multiply(sum_seq1, sum_seq2), n_bins_sq))\n","    x_coef = np.divide(covar, np.multiply(std_seq1, std_seq2))\n","    x_coef = np.real(x_coef)\n","    x_coef = np.nan_to_num(x_coef)\n","    return x_coef\n","\n","  def rotated_sacs(self, sac, angles):\n","    return [\n","        scipy.ndimage.interpolation.rotate(sac, angle, reshape=False)\n","        for angle in angles\n","    ]\n","\n","  def get_grid_scores_for_mask(self, sac, rotated_sacs, mask):\n","    \"\"\"Calculate Pearson correlations of area inside mask at corr_angles.\"\"\"\n","    masked_sac = sac * mask\n","    ring_area = np.sum(mask)\n","    # Calculate dc on the ring area\n","    masked_sac_mean = np.sum(masked_sac) / ring_area\n","    # Center the sac values inside the ring\n","    masked_sac_centered = (masked_sac - masked_sac_mean) * mask\n","    variance = np.sum(masked_sac_centered**2) / ring_area + 1e-5\n","    corrs = dict()\n","    for angle, rotated_sac in zip(self._corr_angles, rotated_sacs):\n","      masked_rotated_sac = (rotated_sac - masked_sac_mean) * mask\n","      cross_prod = np.sum(masked_sac_centered * masked_rotated_sac) / ring_area\n","      corrs[angle] = cross_prod / variance\n","    return self.grid_score_60(corrs), self.grid_score_90(corrs), variance\n","\n","  def get_scores(self, rate_map):\n","    \"\"\"Get summary of scrores for grid cells.\"\"\"\n","    sac = self.calculate_sac(rate_map)\n","    rotated_sacs = self.rotated_sacs(sac, self._corr_angles)\n","\n","    scores = [\n","        self.get_grid_scores_for_mask(sac, rotated_sacs, mask)\n","        for mask, mask_params in self._masks  # pylint: disable=unused-variable\n","    ]\n","    scores_60, scores_90, variances = map(np.asarray, zip(*scores))  # pylint: disable=unused-variable\n","    max_60_ind = np.argmax(scores_60)\n","    max_90_ind = np.argmax(scores_90)\n","\n","    return (scores_60[max_60_ind], scores_90[max_90_ind],\n","            self._masks[max_60_ind][1], self._masks[max_90_ind][1], sac)\n","\n","  def plot_ratemap(self, ratemap, ax=None, title=None, *args, **kwargs):  # pylint: disable=keyword-arg-before-vararg\n","    \"\"\"Plot ratemaps.\"\"\"\n","    if ax is None:\n","      ax = plt.gca()\n","    # Plot the ratemap\n","    ax.imshow(ratemap, interpolation='none', *args, **kwargs)\n","    # ax.pcolormesh(ratemap, *args, **kwargs)\n","    ax.axis('off')\n","    if title is not None:\n","      ax.set_title(title)\n","\n","  def plot_sac(self,\n","               sac,\n","               mask_params=None,\n","               ax=None,\n","               title=None,\n","               *args,\n","               **kwargs):  # pylint: disable=keyword-arg-before-vararg\n","    \"\"\"Plot spatial autocorrelogram.\"\"\"\n","    if ax is None:\n","      ax = plt.gca()\n","    # Plot the sac\n","    useful_sac = sac * self._plotting_sac_mask\n","    ax.imshow(useful_sac, interpolation='none', *args, **kwargs)\n","    # ax.pcolormesh(useful_sac, *args, **kwargs)\n","    # Plot a ring for the adequate mask\n","    if mask_params is not None:\n","      center = self._nbins - 1\n","      ax.add_artist(\n","          plt.Circle(\n","              (center, center),\n","              mask_params[0] * self._nbins,\n","              # lw=bump_size,\n","              fill=False,\n","              edgecolor='k'))\n","      ax.add_artist(\n","          plt.Circle(\n","              (center, center),\n","              mask_params[1] * self._nbins,\n","              # lw=bump_size,\n","              fill=False,\n","              edgecolor='k'))\n","    ax.axis('off')\n","    if title is not None:\n","      ax.set_title(title)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n","  return f(*args, **kwds)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"nhH_ODh7D1xk","colab_type":"text"},"source":["# utils.py"]},{"cell_type":"code","metadata":{"id":"_yrLWdxLDLsN","colab_type":"code","colab":{}},"source":["# Copyright 2018 Google LLC\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\n","\"\"\"Helper functions for creating the training graph and plotting.\n","\"\"\"\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import os\n","from matplotlib.backends.backend_pdf import PdfPages\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","\n","# import ensembles  # pylint: disable=g-bad-import-order\n","\n","\n","np.seterr(invalid=\"ignore\")\n","\n","\n","def get_place_cell_ensembles(\n","    env_size, neurons_seed, targets_type, lstm_init_type, n_pc, pc_scale):\n","  \"\"\"Create the ensembles for the Place cells.\"\"\"\n","  place_cell_ensembles = [\n","      PlaceCellEnsemble(\n","          n,\n","          stdev=s,\n","          pos_min=-env_size / 2.0,\n","          pos_max=env_size / 2.0,\n","          seed=neurons_seed,\n","          soft_targets=targets_type,\n","          soft_init=lstm_init_type)\n","      for n, s in zip(n_pc, pc_scale)\n","  ]\n","  return place_cell_ensembles\n","\n","\n","def get_head_direction_ensembles(\n","    neurons_seed, targets_type, lstm_init_type, n_hdc, hdc_concentration):\n","  \"\"\"Create the ensembles for the Head direction cells.\"\"\"\n","  head_direction_ensembles = [\n","      HeadDirectionCellEnsemble(\n","          n,\n","          concentration=con,\n","          seed=neurons_seed,\n","          soft_targets=targets_type,\n","          soft_init=lstm_init_type)\n","      for n, con in zip(n_hdc, hdc_concentration)\n","  ]\n","  return head_direction_ensembles\n","\n","\n","def encode_initial_conditions(init_pos, init_hd, place_cell_ensembles,\n","                              head_direction_ensembles):\n","  initial_conds = []\n","  for ens in place_cell_ensembles:\n","    initial_conds.append(\n","        tf.squeeze(ens.get_init(init_pos[:, tf.newaxis, :]), axis=1))\n","  for ens in head_direction_ensembles:\n","    initial_conds.append(\n","        tf.squeeze(ens.get_init(init_hd[:, tf.newaxis, :]), axis=1))\n","  return initial_conds\n","\n","\n","def encode_targets(target_pos, target_hd, place_cell_ensembles,\n","                   head_direction_ensembles):\n","  ensembles_targets = []\n","  for ens in place_cell_ensembles:\n","    ensembles_targets.append(ens.get_targets(target_pos))\n","  for ens in head_direction_ensembles:\n","    ensembles_targets.append(ens.get_targets(target_hd))\n","  return ensembles_targets\n","\n","\n","def clip_all_gradients(g, var, limit):\n","  # print(var.name)\n","  return (tf.clip_by_value(g, -limit, limit), var)\n","\n","\n","def clip_bottleneck_gradient(g, var, limit):\n","  if (\"bottleneck\" in var.name or \"pc_logits\" in var.name):\n","    return (tf.clip_by_value(g, -limit, limit), var)\n","  else:\n","    return (g, var)\n","\n","\n","def no_clipping(g, var):\n","  return (g, var)\n","\n","\n","def concat_dict(acc, new_data):\n","  \"\"\"Dictionary concatenation function.\"\"\"\n","\n","  def to_array(kk):\n","    if isinstance(kk, np.ndarray):\n","      return kk\n","    else:\n","      return np.asarray([kk])\n","\n","  for k, v in new_data.iteritems():\n","    if isinstance(v, dict):\n","      if k in acc:\n","        acc[k] = concat_dict(acc[k], v)\n","      else:\n","        acc[k] = concat_dict(dict(), v)\n","    else:\n","      v = to_array(v)\n","      if k in acc:\n","        acc[k] = np.concatenate([acc[k], v])\n","      else:\n","        acc[k] = np.copy(v)\n","  return acc\n","\n","\n","def get_scores_and_plot(scorer,\n","                        data_abs_xy,\n","                        activations,\n","                        directory,\n","                        filename,\n","                        plot_graphs=True,  # pylint: disable=unused-argument\n","                        nbins=20,  # pylint: disable=unused-argument\n","                        cm=\"jet\",\n","                        sort_by_score_60=True):\n","  \"\"\"Plotting function.\"\"\"\n","\n","  # Concatenate all trajectories\n","  xy = data_abs_xy.reshape(-1, data_abs_xy.shape[-1])\n","  act = activations.reshape(-1, activations.shape[-1])\n","  n_units = act.shape[1]\n","  # Get the rate-map for each unit\n","  s = [\n","      scorer.calculate_ratemap(xy[:, 0], xy[:, 1], act[:, i])\n","      for i in xrange(n_units)\n","  ]\n","  # Get the scores\n","  score_60, score_90, max_60_mask, max_90_mask, sac = zip(\n","      *[scorer.get_scores(rate_map) for rate_map in s])\n","  # Separations\n","  # separations = map(np.mean, max_60_mask)\n","  # Sort by score if desired\n","  if sort_by_score_60:\n","    ordering = np.argsort(-np.array(score_60))\n","  else:\n","    ordering = range(n_units)\n","  # Plot\n","  cols = 16\n","  rows = int(np.ceil(n_units / cols))\n","  fig = plt.figure(figsize=(24, rows * 4))\n","  for i in xrange(n_units):\n","    rf = plt.subplot(rows * 2, cols, i + 1)\n","    acr = plt.subplot(rows * 2, cols, n_units + i + 1)\n","    if i < n_units:\n","      index = ordering[i]\n","      title = \"%d (%.2f)\" % (index, score_60[index])\n","      # Plot the activation maps\n","      scorer.plot_ratemap(s[index], ax=rf, title=title, cmap=cm)\n","      # Plot the autocorrelation of the activation maps\n","      scorer.plot_sac(\n","          sac[index],\n","          mask_params=max_60_mask[index],\n","          ax=acr,\n","          title=title,\n","          cmap=cm)\n","  # Save\n","  if not os.path.exists(directory):\n","    os.makedirs(directory)\n","  with PdfPages(os.path.join(directory, filename), \"w\") as f:\n","    plt.savefig(f, format=\"pdf\")\n","  plt.close(fig)\n","  return (np.asarray(score_60), np.asarray(score_90),\n","          np.asarray(map(np.mean, max_60_mask)),\n","          np.asarray(map(np.mean, max_90_mask)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-gVlG0qD690","colab_type":"text"},"source":["# train.py"]},{"cell_type":"code","metadata":{"id":"jc1w1LvzgI5w","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_8fL545wC2Sd","colab_type":"code","outputId":"a33bf56d-47d7-4968-e546-432e6ebede42","executionInfo":{"status":"error","timestamp":1573577244870,"user_tz":300,"elapsed":1135,"user":{"displayName":"Salim M'Jahad","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCFL4tbM0hSOkSuMWfn2CYHleHxlhi9cylY8gPIOQ=s64","userId":"06219843532523503157"}},"colab":{"base_uri":"https://localhost:8080/","height":395}},"source":["# Copyright 2018 Google LLC\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\n","\"\"\"Supervised training for the Grid cell network.\"\"\"\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import matplotlib\n","import numpy as np\n","import tensorflow as tf\n","import tkinter  # pylint: disable=unused-import\n","\n","matplotlib.use('Agg')\n","\n","# import dataset_reader  # pylint: disable=g-bad-import-order, g-import-not-at-top\n","# import model  # pylint: disable=g-bad-import-order\n","# import scores  # pylint: disable=g-bad-import-order\n","# import utils  # pylint: disable=g-bad-import-order\n","\n","\n","# Task config\n","tf.flags.DEFINE_string('task_dataset_info', 'square_room',\n","                       'Name of the room in which the experiment is performed.')\n","tf.flags.DEFINE_string('task_root',\n","                       None,\n","                       'Dataset path.')\n","tf.flags.DEFINE_float('task_env_size', 2.2,\n","                      'Environment size (meters).')\n","tf.flags.DEFINE_list('task_n_pc', [256],\n","                     'Number of target place cells.')\n","tf.flags.DEFINE_list('task_pc_scale', [0.01],\n","                     'Place cell standard deviation parameter (meters).')\n","tf.flags.DEFINE_list('task_n_hdc', [12],\n","                     'Number of target head direction cells.')\n","tf.flags.DEFINE_list('task_hdc_concentration', [20.],\n","                     'Head direction concentration parameter.')\n","tf.flags.DEFINE_integer('task_neurons_seed', 8341,\n","                        'Seeds.')\n","tf.flags.DEFINE_string('task_targets_type', 'softmax',\n","                       'Type of target, soft or hard.')\n","tf.flags.DEFINE_string('task_lstm_init_type', 'softmax',\n","                       'Type of LSTM initialisation, soft or hard.')\n","tf.flags.DEFINE_bool('task_velocity_inputs', True,\n","                     'Input velocity.')\n","tf.flags.DEFINE_list('task_velocity_noise', [0.0, 0.0, 0.0],\n","                     'Add noise to velocity.')\n","\n","# Model config\n","tf.flags.DEFINE_integer('model_nh_lstm', 128, 'Number of hidden units in LSTM.')\n","tf.flags.DEFINE_integer('model_nh_bottleneck', 256,\n","                        'Number of hidden units in linear bottleneck.')\n","tf.flags.DEFINE_list('model_dropout_rates', [0.5],\n","                     'List of floats with dropout rates.')\n","tf.flags.DEFINE_float('model_weight_decay', 1e-5,\n","                      'Weight decay regularisation')\n","tf.flags.DEFINE_bool('model_bottleneck_has_bias', False,\n","                     'Whether to include a bias in linear bottleneck')\n","tf.flags.DEFINE_float('model_init_weight_disp', 0.0,\n","                      'Initial weight displacement.')\n","\n","# Training config\n","tf.flags.DEFINE_integer('training_epochs', 1000, 'Number of training epochs.')\n","tf.flags.DEFINE_integer('training_steps_per_epoch', 1000,\n","                        'Number of optimization steps per epoch.')\n","tf.flags.DEFINE_integer('training_minibatch_size', 10,\n","                        'Size of the training minibatch.')\n","tf.flags.DEFINE_integer('training_evaluation_minibatch_size', 4000,\n","                        'Size of the minibatch during evaluation.')\n","tf.flags.DEFINE_string('training_clipping_function', 'clip_all_gradients',\n","                       'Function for gradient clipping.')\n","tf.flags.DEFINE_float('training_clipping', 1e-5,\n","                      'The absolute value to clip by.')\n","\n","tf.flags.DEFINE_string('training_optimizer_class', 'tf.train.RMSPropOptimizer',\n","                       'The optimizer used for training.')\n","tf.flags.DEFINE_string('training_optimizer_options',\n","                       '{\"learning_rate\": 1e-5, \"momentum\": 0.9}',\n","                       'Defines a dict with opts passed to the optimizer.')\n","\n","# Store\n","tf.flags.DEFINE_string('saver_results_directory',\n","                       None,\n","                       'Path to directory for saving results.')\n","tf.flags.DEFINE_integer('saver_eval_time', 2,\n","                        'Frequency at which results are saved.')\n","\n","# Options\n","tf.flags.DEFINE_string('task_root',\n","                       './grid-cells-datasets',\n","                       'Root directory')\n","tf.flags.DEFINE_string('saver_results_directory',\n","                       './results',\n","                        'Results directory.')\n","\n","# Require flags\n","tf.flags.mark_flag_as_required('task_root')\n","tf.flags.mark_flag_as_required('saver_results_directory')\n","FLAGS = tf.flags.FLAGS\n","\n","\n","def train():\n","  \"\"\"Training loop.\"\"\"\n","\n","  tf.reset_default_graph()\n","\n","  # Create the motion models for training and evaluation\n","  data_reader = DataReader(\n","      FLAGS.task_dataset_info, root=FLAGS.task_root, num_threads=4)\n","  train_traj = read(batch_size=FLAGS.training_minibatch_size)\n","\n","  # Create the ensembles that provide targets during training\n","  place_cell_ensembles = get_place_cell_ensembles(\n","      env_size=FLAGS.task_env_size,\n","      neurons_seed=FLAGS.task_neurons_seed,\n","      targets_type=FLAGS.task_targets_type,\n","      lstm_init_type=FLAGS.task_lstm_init_type,\n","      n_pc=FLAGS.task_n_pc,\n","      pc_scale=FLAGS.task_pc_scale)\n","\n","  head_direction_ensembles = get_head_direction_ensembles(\n","      neurons_seed=FLAGS.task_neurons_seed,\n","      targets_type=FLAGS.task_targets_type,\n","      lstm_init_type=FLAGS.task_lstm_init_type,\n","      n_hdc=FLAGS.task_n_hdc,\n","      hdc_concentration=FLAGS.task_hdc_concentration)\n","  target_ensembles = place_cell_ensembles + head_direction_ensembles\n","\n","  # Model creation\n","  rnn_core = GridCellsRNNCell(\n","      target_ensembles=target_ensembles,\n","      nh_lstm=FLAGS.model_nh_lstm,\n","      nh_bottleneck=FLAGS.model_nh_bottleneck,\n","      dropoutrates_bottleneck=np.array(FLAGS.model_dropout_rates),\n","      bottleneck_weight_decay=FLAGS.model_weight_decay,\n","      bottleneck_has_bias=FLAGS.model_bottleneck_has_bias,\n","      init_weight_disp=FLAGS.model_init_weight_disp)\n","  rnn = GridCellsRNN(rnn_core, FLAGS.model_nh_lstm)\n","\n","  # Get a trajectory batch\n","  input_tensors = []\n","  init_pos, init_hd, ego_vel, target_pos, target_hd = train_traj\n","  if FLAGS.task_velocity_inputs:\n","    # Add the required amount of noise to the velocities\n","    vel_noise = tf.distributions.Normal(0.0, 1.0).sample(\n","        sample_shape=ego_vel.get_shape()) * FLAGS.task_velocity_noise\n","    input_tensors = [ego_vel + vel_noise] + input_tensors\n","  # Concatenate all inputs\n","  inputs = tf.concat(input_tensors, axis=2)\n","\n","  # Replace euclidean positions and angles by encoding of place and hd ensembles\n","  # Note that the initial_conds will be zeros if the ensembles were configured\n","  # to provide that type of initialization\n","  initial_conds = encode_initial_conditions(\n","      init_pos, init_hd, place_cell_ensembles, head_direction_ensembles)\n","\n","  # Encode targets as well\n","  ensembles_targets = encode_targets(\n","      target_pos, target_hd, place_cell_ensembles, head_direction_ensembles)\n","\n","  # Estimate future encoding of place and hd ensembles inputing egocentric vels\n","  outputs, _ = rnn(initial_conds, inputs, training=True)\n","  ensembles_logits, bottleneck, lstm_output = outputs\n","\n","  # Training loss\n","  pc_loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n","      labels=ensembles_targets[0], logits=ensembles_logits[0], name='pc_loss')\n","  hd_loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n","      labels=ensembles_targets[1], logits=ensembles_logits[1], name='hd_loss')\n","  total_loss = pc_loss + hd_loss\n","  train_loss = tf.reduce_mean(total_loss, name='train_loss')\n","\n","  # Optimisation ops\n","  optimizer_class = eval(FLAGS.training_optimizer_class)  # pylint: disable=eval-used\n","  optimizer = optimizer_class(**eval(FLAGS.training_optimizer_options))  # pylint: disable=eval-used\n","  grad = optimizer.compute_gradients(train_loss)\n","  clip_gradient = eval(FLAGS.training_clipping_function)  # pylint: disable=eval-used\n","  clipped_grad = [\n","      clip_gradient(g, var, FLAGS.training_clipping) for g, var in grad\n","  ]\n","  train_op = optimizer.apply_gradients(clipped_grad)\n","\n","  # Store the grid scores\n","  grid_scores = dict()\n","  grid_scores['btln_60'] = np.zeros((FLAGS.model_nh_bottleneck,))\n","  grid_scores['btln_90'] = np.zeros((FLAGS.model_nh_bottleneck,))\n","  grid_scores['btln_60_separation'] = np.zeros((FLAGS.model_nh_bottleneck,))\n","  grid_scores['btln_90_separation'] = np.zeros((FLAGS.model_nh_bottleneck,))\n","  grid_scores['lstm_60'] = np.zeros((FLAGS.model_nh_lstm,))\n","  grid_scores['lstm_90'] = np.zeros((FLAGS.model_nh_lstm,))\n","\n","  # Create scorer objects\n","  starts = [0.2] * 10\n","  ends = np.linspace(0.4, 1.0, num=10)\n","  masks_parameters = zip(starts, ends.tolist())\n","  latest_epoch_scorer = GridScorer(20, data_reader.get_coord_range(),\n","                                          masks_parameters)\n","\n","  with tf.train.SingularMonitoredSession() as sess:\n","    for epoch in range(FLAGS.training_epochs):\n","      loss_acc = list()\n","      for _ in range(FLAGS.training_steps_per_epoch):\n","        res = sess.run({'train_op': train_op, 'total_loss': train_loss})\n","        loss_acc.append(res['total_loss'])\n","\n","      tf.logging.info('Epoch %i, mean loss %.5f, std loss %.5f', epoch,\n","                      np.mean(loss_acc), np.std(loss_acc))\n","      if epoch % FLAGS.saver_eval_time == 0:\n","        res = dict()\n","        for _ in xrange(FLAGS.training_evaluation_minibatch_size //\n","                        FLAGS.training_minibatch_size):\n","          mb_res = sess.run({\n","              'bottleneck': bottleneck,\n","              'lstm': lstm_output,\n","              'pos_xy': target_pos\n","          })\n","          res = concat_dict(res, mb_res)\n","\n","        # Store at the end of validation\n","        filename = 'rates_and_sac_latest_hd.pdf'\n","        grid_scores['btln_60'], grid_scores['btln_90'], grid_scores[\n","            'btln_60_separation'], grid_scores[\n","                'btln_90_separation'] = get_scores_and_plot(\n","                    latest_epoch_scorer, res['pos_xy'], res['bottleneck'],\n","                    FLAGS.saver_results_directory, filename)\n","\n","\n","def main(unused_argv):\n","  tf.logging.set_verbosity(3)  # Print INFO log messages.\n","  train()\n","\n","if __name__ == '__main__':\n","  tf.app.run()"],"execution_count":24,"outputs":[{"output_type":"error","ename":"DuplicateFlagError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-9168e52cbecb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m tf.flags.DEFINE_string('task_root',\n\u001b[1;32m     89\u001b[0m                        \u001b[0;34m'./grid-cells-datasets'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                        'Root directory')\n\u001b[0m\u001b[1;32m     91\u001b[0m tf.flags.DEFINE_string('saver_results_directory',\n\u001b[1;32m     92\u001b[0m                        \u001b[0;34m'./results'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_string\u001b[0;34m(name, default, help, flag_values, **args)\u001b[0m\n\u001b[1;32m    239\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m   \u001b[0mDEFINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[1;32m     80\u001b[0m   \"\"\"\n\u001b[1;32m     81\u001b[0m   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\n\u001b[0;32m---> 82\u001b[0;31m               flag_values, module_name)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'task_root' is defined twice. First from /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py, Second from /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py.  Description from first occurrence: Dataset path."]}]},{"cell_type":"code","metadata":{"id":"P-iXzDYshDuv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"583903f0-56fb-44ec-d15e-4fc245009dbd","executionInfo":{"status":"ok","timestamp":1573577318240,"user_tz":300,"elapsed":3717,"user":{"displayName":"Salim M'Jahad","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCFL4tbM0hSOkSuMWfn2CYHleHxlhi9cylY8gPIOQ=s64","userId":"06219843532523503157"}}},"source":["!git clone https://github.com/deepmind/grid-cells.git"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Cloning into 'grid-cells'...\n","remote: Enumerating objects: 13, done.\u001b[K\n","remote: Total 13 (delta 0), reused 0 (delta 0), pack-reused 13\u001b[K\n","Unpacking objects: 100% (13/13), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SEpNeAdXDCE7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"aa5ced5c-af29-4b72-b3a1-e1fa81f7b1d2","executionInfo":{"status":"ok","timestamp":1573577349126,"user_tz":300,"elapsed":2800,"user":{"displayName":"Salim M'Jahad","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCFL4tbM0hSOkSuMWfn2CYHleHxlhi9cylY8gPIOQ=s64","userId":"06219843532523503157"}}},"source":["!ls grid-cells"],"execution_count":27,"outputs":[{"output_type":"stream","text":["CONTRIBUTING.md    ensembles.py  model.py   scores.py  utils.py\n","dataset_reader.py  LICENSE\t README.md  train.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H2XvrXNlhOi6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}